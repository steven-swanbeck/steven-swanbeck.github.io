<!DOCTYPE html>
<!-- <body style="background-color:rgb(230, 196, 255);">
    <h1>HTML Articles</h1>
</body> -->
<html>

<head>
    <title>Steven Swanbeck</title>
    <meta charset="UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>
    <meta name="description" content="The personal website and online portfolio of Steven Swanbeck, a robotics PhD student at The University of Texas at Austin."/>
    <meta name="keywords" content="steven swanbeck, robotics, machine learning, artifical intelligence, ut austin, nuclear and applied robotics group"/>
    <meta name="google-site-verification" content="_84jKoQeQmdGWScBpFXvMaCmgxhsOYs4yGrtqFKsEz0" />

    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@200;300;400;600;700;900&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="assets/css/style.css" />
    <link rel="icon" type="image/x-icon" href="assets/images/ut_shield.png">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-LDSQMLZ3GJ"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-LDSQMLZ3GJ');
    </script> -->
    <title> Steven Swanbeck </title>
</head>

<body>
    <div style="width:1000px;margin: 0px auto;">
        <header id="header" width="400px" style="display:flex;justify-content: space-around;">
            <a href="#profile-intro">Home</a>
            <a href="#major-updates">Major Updates</a>
            <a href="#projects">Projects</a>
        </header>
        <div id="profile">
            <div id="profile-pic">
                <img src="assets/images/profile.jpg" />
            </div>
            <div id="profile-intro">
                <div id="profile-name">Steven Swanbeck</div>
                <div id="profile-email">stevenswanbeck at gmail dot com</div>
                <p align="justify">
                    I am pursuing my PhD in Mechanical Engineering and Robotics at The University of Texas at Austin advised by
                    <a href="https://www.me.utexas.edu/people/faculty-directory/pryor">Dr. Mitch Pryor</a> in the <a href="https://robotics.me.utexas.edu/">Nuclear and Applied Robotics Group (NRG)</a>.
                    I am passionate about developing robotic systems that assist humans to solve our greatest challenges. I am fascinated my many facets of robotics and have broad experience in perception, manipulation, and human-robot interaction. As of now, I am most interested in developing generalized robotics skillsets and creating behavior tree synthesis and validation mechanisms and software deployment infrastructure to allow these skillsets to be used to complete any task on any system.
                </p>
                <p align="justify">
                    Previously, I completed my BS in Mechanical Engineering from the University of Nevada, Reno, advised by <a href="https://www.unr.edu/me/about/people/jun-zhang">Dr. Jun Zhang</a>
                    in the <a href="https://sites.google.com/view/unrsml/home">Smart Robotics Lab</a>.
                </p>
                <p align="justify">
                    Aside from robotics, I also enjoy basketball, rock climbing, origami, puzzles, and sci-fi novels.
                </p>
            
                <div>
                    <!-- <a href="assets/files/Resume.pdf">
                      Resume
                    </a>
                    / -->
                    <a href="https://www.linkedin.com/in/stevenswanbeck">
                      LinkedIn
                    </a>
                    /
                    <a href="https://github.com/steven-swanbeck">
                      Github
                    </a>
                    </a>
                </div>
            </div>
            <div style="clear: both;"></div>
        </div>
        <div class="section" id="major-updates">
            <h1>Major Updates</h1>
            <ul>
                <li> <b>Oct 2024</b> Motivated by the directions of my PhD research, I have founded <i>Sirus Robotics</i>. More information to follow at a later date.
                <li> <b>Aug 2022</b> I'm starting my PhD in robotics at UT Austin in the <a href="https://robotics.me.utexas.edu/">Nuclear and Applied Robotics Group</a> advised by Dr. Mitch Pryor.
                <li> <b>Dec 2021</b> I graduated Summa Cum Laude as the valedictorian of the engineering class with my BS from UNR.</li>
                <li> <b>Aug 2021</b> I'm starting research with the <a href="https://packpages.unr.edu/jun">Smart Robotics Lab</a> advised by Dr. Jun Zhang.</li>
                <li> <b>May 2021</b> I completed my first robotics class, Dr. Jun Zhang's ME422 Introduction to Robotics, and pivoted my aspirations from aerospace engineering toward robotics.</li>
            </ul>
            <p></p>
            <div style="clear: both;"></div>
        </div>
        <div class="divider"></div>
        <div style="display:flex;flex-direction: column;" id="projects">
            <h1>Projects</h1>
            
            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/darwin_video.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    Darwin: An Ecosystem of Modular and Flexible Skillsets for Robotics Applications
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Research Project &nbsp;&nbsp;&nbsp;&nbsp; <br>
                    <b>Oct 2024 - Present</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="">Code (coming soon)</a>
                    <li><i>Darwin</i> is a paradigm for creating maximally modular robotics skillsets. Each skillset is a <a href="https://docs.ros.org/en/humble/index.html">ROS</a>-enabled, <a href="https://www.behaviortree.dev/">BT.CPP</a>-driven, and <a href="https://www.docker.com/">Docker</a>-containerized application.</li>
                    <li>Using ROS allows for communication between each containerized skillset and use of behavior trees allows skills to be easily chained together to produce complex, task-specific, and reactive system behavior. Containerization allows for easy deployment across many different systems of different types.</li>
                    <li>Current <i>Darwin</i> capabilities include visual sensor fusion, 3D SLAM, data aggregation, ML model integration, information gain viewpose sampling, generative behavior tree construction and validation, raw data annotation, Meta's <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Llama 3</a> and <a href="https://ai.meta.com/sam2/">SAM 2</a>, and corrosion detection, plus many supported robot and sensor drivers. New capabilities will be continuously added for the foreseeable future.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/hyla_slam_video.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    Hyla-SLAM: Extremely Scalable and Memory Efficient LiDAR-Based SLAM for Robotics Applications
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Research Project &nbsp;&nbsp;&nbsp;&nbsp; <br>
                    <b>Aug 2024 - Present</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="">Paper (coming soon)</a>
                    <li><i>Hyla-SLAM</i> integrates the dynamic data loading properties of the Hylacomylus mapping plugin with a set of possible localization solutions (the default of which is <a href="https://github.com/PRBonn/kiss-icp">KISS-ICP</a>) to provide a scalable SLAM solution that can build infinite* maps and can be deployed on any system that has a 3D LiDAR sensor.</li>
                    <li>*Can generate maps with volume up to cube with side length of 20,000 light years. Dynamic data loading means that the memory profile remains roughly constant during operation, preventing progressive performance degradation as map size grows.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/mabttb_video.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    The BT.CPP Multi-Agent Behavior Tree Toolbox
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Research Project &nbsp;&nbsp;&nbsp;&nbsp; <br>
                    <b>Aug 2024 - Oct 2024</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="">Code (coming soon)</a>
                    <li>Behavior trees are incredibly popular for use in robotics applications because of their flexibilty in construction and modularity. However, they do not generally scale to decentralized multi-agent applications. </li>
                    <li>The <i>Multi-Agent Behavior Tree Toolbox</i> aims to solve this by providing a set of general-purpose
                    <a href="https://www.behaviortree.dev/">BT.CPP</a>behaviors for decentralized coordination and communication between multiple systems (using ROS), transfer of data on disk between systems (using a lightweight data transfer library written in Rust), and running of subprocesses.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/poirot_video.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    Poirot: General-Purpose Behavior Tree Interfaces for Machine Learning Models
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Research Project &nbsp;&nbsp;&nbsp;&nbsp; <br>
                    <b>Aug 2024 - Oct 2024</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="">Code (coming soon)</a>
                    <li><i>Poirot</i> is designed to make integration of machine learning capabilities into larger behavior trees simple.</li>
                    <li><i>Poirot</i> standardizes input and output types of models of interest and provides general-purpose ROS wrappers and BT.CPP behaviors to make it easy to integrate <b>any</b> machine learning model or similar input-output process into a larger robot software ecosystem.</li>
                    <li>Plugins currently exist for Meta's <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Llama 3</a> and <a href="https://ai.meta.com/sam2/">SAM 2</a> models, IDEA-Research's <a href="https://github.com/IDEA-Research/Grounded-SAM-2">Grounded SAM 2</a>, <a href="https://github.com/wkentaro/labelme">LabelMe</a>, and other custom models.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/surface_repair2_thumbnail.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    A Scalable SLAM and Object Extraction Pipeline for Informing Contact and Non-Contact Manipulation Tasks
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Research Project &nbsp;&nbsp;&nbsp;&nbsp; <br>
                    <b>Jan 2024 - Present</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/steven-swanbeck/autonomous_robots">Code (coming soon)</a>
                    <li>Supports custom plugins for image-based predictions and supports fusion of image data with depth images and point clouds generated by robot sensors.</li>
                    <li>A grounded Segment Anything plugin is implemented to extract objects using text inputs.</li>
                    <Supports>Using behavior trees, the processes associated with data collection, annotation, fusion, and storage can be easily adjusted to fit a desired task, and the stack supports any number of visual data sensors out of the box (subject to computational constraints).</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/autonomous_robots_summary.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="https://github.com/steven-swanbeck/autonomous_robots" class="research-proj-title">
                    Autonomous Obstacle Avoidance, Localization, Navigation, and SLAM on a Mobile Robot
                </a>
                <p>
                    <b>Steven Swanbeck</b>,
                    <a style="color:#000;" href="">Daniel Meza,</a>
                    <a style="color:#000;" href="">Jared Rosenbaum</a>
                    <br> Course Project &nbsp;&nbsp;&nbsp;&nbsp; <br>
                    <b>Jan 2024 - Apr 2024</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/steven-swanbeck/autonomous_robots">Code</a>
                    <li>Simple obstacle avoidance, particle filter-based localization, navigation using RRT* for global planning and a line-of-sight carrot planner for local planning, and SLAM using correlative scan matching and GTSAM for pose graph optimization implemented to run onboard an autonomous mobile robot in real time.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/hylacomylus_example.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    Hylacomylus: Memory Efficient Mapping for Operation in Large-Scale Environments
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Research Project &nbsp;&nbsp;&nbsp;&nbsp; <br>
                    <b>Jan 2024 - Apr 2024</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="">Code (coming soon)</a>
                    <!-- <li><i>Hylacomylus</i> implements pure visual SLAM using <a href="https://github.com/PRBonn/kiss-icp">KISS-ICP</a> with dynamic data loading and unloading properties to allow comutationally-limited robots to produce and maintain highly-detailed maps of space that remain computationally- and memory-friendly.</li> -->
                    <li><i>Hylacomylus</i> is a mapping plugin that enables dynamic data loading and unloading during robot deployment. This allows computationally- or memory-constrained robots to produce and maintain maps that are both extremely dense and extremely large.</li>
                    <li>Produced maps can be effortlessly reloaded into working memory, transferred to other systems for use, or extended on subsequient deployments.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/gators_square.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    GaTORS: A Game-Theoretic Tool for Optimal Robot Platform Selection and Design in Surface Coverage Applications 
                </a>
                <p>
                    <b>Steven Swanbeck</b>,
                    <a style="color:#000;" href="">Daniel Meza,</a>
                    <a style="color:#000;" href="">Jared Rosenbaum,</a>
                    <a style="color:#000;" href="">David Fridovich-Keil,</a>
                    <a style="color:#000;" href="">Mitch Pryor</a>

                    <!-- <br> Submitted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024) &nbsp;&nbsp;&nbsp;&nbsp; <br> -->
                    <br> Research Project &nbsp;&nbsp;&nbsp;&nbsp; <br>
                    <b>Jan 2024 - Mar 2024</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="">Paper (coming soon)</a>
                    <li>GaTORS presents a framework that helps answer the questions of <i>which</i> and <i>how many</i> robots should be deployed in real world environments to complete surface coverage tasks.</li>
                    <li>The design of systems can also be optimized by performing systematic parameter sweeps over the space of possible configurations to set targets to achieve economic viability.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/ar_star_interactions.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    AR-STAR: An Augmented Reality Tool for Online Modification of Robot Point Cloud Data
                </a>
                <p>
                    <a style="color:#000;" href="">Frank Regal*,</a>
                    <b>Steven Swanbeck*</b>,
                    <a style="color:#000;" href="">Fabian Parra,</a>
                    <a style="color:#000;" href="">Mitch Pryor</a>
                    <br> ACM/IEEE International Conference on Human-Robot Interaction (HRI '24) &nbsp;&nbsp;&nbsp;&nbsp; <br>
                    <b>Oct 2023 - Dec 2023</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://dl.acm.org/doi/10.1145/3610978.3640571">Paper</a>
                    <li>Uncertainty in sensor data collected onboard a deployed robot can hinder its ability to complete a mission, and it can be difficult for a robot to autonomously detect and overcome these uncertainties.</li>
                    <li>We develop an augmented reality tool that allows a a human supervisor to visualize a robot's predictions and modify them online using one of three interaction modalities and study user preferences in using them.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/magneto_rl.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    Reinforcement Learning for Traversal of Uncertain Vertical Terrain using a Magnetic Wall-Climbing Robot 
                </a>
                <p>
                    <b>Steven Swanbeck</b>,
                    <a style="color:#000;" href="">Jee-Eun Lee</a>
                    <br> Course Project &nbsp;&nbsp;&nbsp;&nbsp; <br>
                    <b>Sep 2023 - Dec 2023</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/steven-swanbeck/magneto_rl.git">Code</a>
                    <li>Using only its own kinematics, magnetic forces experienced at its feet, and a goal position, a magnetic wall climbing robot is trained to navigate a surface within unknown and irregular magnetic properties.</li>
                    <li>Developed a custom ROS-based bridge to interface the C++ simulation toolkit DART with Python RL libraries and trained Deep Q-Learning and PPO policies using it.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/we_paint.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    Game-Theoretic Modeling for Robot Platform Selection in Industrial Repair Applications 
                </a>
                <p>
                    <b>Steven Swanbeck</b>,
                    <a style="color:#000;" href="">Daniel Meza</a>
                    <br> Course Project &nbsp;&nbsp;&nbsp;&nbsp; <br>
                    <b>Oct 2023 - Dec 2023</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/steven-swanbeck/game_theoretic_painting.git">Code</a>
                    <li>Using game-theoretic modeling, the ability of different robotic hardware platforms to perform maintenance and inspection tasks in a shared environment is evaluated.</li>
                    <li>With this competitive modeling approach, the ability to select a minimally-sized heterogeneous robot team to accomplish comprehensive corrosion repairs within predefined budget and time constraints in complex industrial environments was demonstrated.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/ar_star.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="https://arxiv.org/abs/2311.00988" class="research-proj-title">
                    Using Augmented Reality to Assess and Modify Mobile Manipulator Surface Repair Plans
                </a>
                <p>
                    <a style="color:#000;" href="https://www.linkedin.com/in/frankregal/">Frank Regal,</a>
                    <b>Steven Swanbeck</b>,
                    <a style="color:#000;" href="">Fabian Parra,</a>
                    <a style="color:#000;" href="">Jared Rosenbaum,</a>
                    <a style="color:#000;" href="https://www.me.utexas.edu/people/faculty-directory/pryor">Mitch Pryor</a>
                    <br> XR-ROB Second International Workshop on "Horizons of Extended Robotics Reality" @ IEEE IROS (2023) &nbsp;&nbsp;&bull;&nbsp;&nbsp; <strong>Second Prize</strong> <br>
                    <b>Jul 2023 - Aug 2023</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://arxiv.org/abs/2311.00988">Paper</a>
                    <li>Using an AR head-mounted display, a user is able to view predictions made by a surveying robot for surface repair.</li>
                    <li>The user can accept, reject, or modify the plan generated by the robot to prevent incidental covering of sensitive material or repair of unproblematic surfaces.</li>
                </p>
            </div>
            
            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/corrosion_repair_demo.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    Non-Contact Surface Coverage of Corroded Material in Industrial Environments
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Research Project <br>
                    <b>May 2023 - Present</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="">Code (coming soon)</a>
                    <li>Surface identification used to extract the locations and geometries of possible corroded surfaces within industrial environments.</li>
                    <li>Coverage planning and execution with constraint-relaxed redundant replanning enables coverage over the identified surfaces using a mobile manipulator system with a protective spray coating, preventing further corrosion development.</li>
                </p>
            </div>
            
            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/virtual_fixtures.mp4" type="video/mp4">
                        </video>
                    </center>
                    </a>
                <a href="" class="research-proj-title">
                    Virtual Fixture Generation and Execution for Surface Coverage of Complex Geometries
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Research Project <br>
                    <b>May 2023 - Jul 2023</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="">Code (coming soon)</a>
                    <li>Using computer vision models, LiDAR detection models, or supervised scene labeling, surfaces on which a robot should perform surface inspection are denoted.</li>
                    <li>A discrete pose mesh is created offset from the surface that can be traversed using a manipulator to perform the surface coverage task.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/passive_drone_gripper_alt.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="https://github.com/steven-swanbeck/passive_tree_gripper/" class="research-proj-title">
                    Bat-Inspired Passive Drone Gripper for Angle-Invariant Perching
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Course Project <br>
                    <b>March 2023 - April 2023</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/steven-swanbeck/passive_tree_gripper/">Code</a>
                    <li>A custom mechanism inspired by the passive inverted hanging ability of bats to allow a drone to remain perched in one location for long periods of time without expending battery power.</li>
                    <li>In addition to perching in upright or inverted orientations, the gripper can also be used as landing gear for the drone or for holding objects during flight.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/ik_toolbox.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="https://github.com/steven-swanbeck/AlgorithmsforSensorBasedRobotics" class="research-proj-title">
                    A MATLAB Toolbox using Screw Theory for Forward and Inverse Kinematics of Manipulators
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Course Project <br>
                    <b>Feb 2023 - April 2023</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/steven-swanbeck/AlgorithmsforSensorBasedRobotics">Code</a>
                    <li>Robots of arbitrary structure can be defined and visualized using screw theory.</li>
                    <li>Space frame and body frame forward kinematics can be calculated and visualized and manipulability measures are calculated and monitored.</li>
                    <li>Various inverse kinematics algorithms, including Jacobian pseudo-inverse, Jacobian transpose, redundancy resolution, and damped least-squares are available.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/detection_fusion_and_pipeline.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    LiDAR & Image Data Fusion for Object Detection with Rapid Labeling and Training Pipeline
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Research Project <br>
                    <b>Jan 2023 - Jun 2023</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="">Code (coming soon)</a>
                    <li>Separate trained LiDAR-based and image-based prediction models are used to make semantic (point-wise/pixel-wise) predictions about the presence of objects of interest within the robot environment.</li>
                    <li>Predictions are fused spatially using projection and probabilistically in the Bayesian sense to predict the locations of these objects in the environment.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/mapping.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    Visual Inspection and Mapping Stack for Industrial Survey Applications
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Research Project <br>
                    <b>Nov 2022 - Mar 2023</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="">Code (coming soon)</a>
                    <li>Custom mapping stack developed to create dense 3D representations of robot surroundings using fused 2D image and 3D LiDAR data.</li>
                    <li>Processing functionalities can be easily implemented to localize and ground regions of interest within the environment; map is also used for robot localization as it is developed.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/game_player.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="https://github.com/steven-swanbeck/robot_shell_game" class="research-proj-title">
                    Robotic Street Scam Artist
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Course Project <br>
                    <b>Oct 2022 - Nov 2022</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/steven-swanbeck/robot_shell_game">Code</a>
                    <li>Combining manipulator control and computer vision, an eye-in-hand system is able to localize and track a series of shells as they are shuffled, inspect each to look for a planted marker, and pick it to reveal the money in a modified version of the classic street scam shell game.</li>
                    <li>Hand-tracking using the manipulator and writing calculation results also developed as intermediate steps.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/modeling.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    Kinematic Modeling of a Twisted-String Actuated Soft Robotic Finger as Part of an Anthropomorphic Gripper
                </a>
                <p>
                    <b>Steven Swanbeck</b>,
                    <a style="color:#000;" href="https://www.linkedin.com/in/revanthraghuram-konda-56bbb2158/">Revanth Konda,</a>
                    <a style="color:#000;" href="https://www.unr.edu/me/about/people/jun-zhang">Jun Zhang</a>
                    <br> Modeling, Estimation, and Control Conference (MECC 2023) <br>
                    <b>Apr 2022 - Aug 2022</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://www.sciencedirect.com/science/article/pii/S240589632302390X">Paper</a>
                    <li>Modeling strategies for a twisted-string actuator-driven soft robotic gripper developed to enable control and autonomous capabilities.</li>
                </p>
            </div>

            <div>
                <a href="https://ieeexplore.ieee.org/abstract/document/10196149" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/star2.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="https://ieeexplore.ieee.org/abstract/document/10196149" class="research-proj-title">
                    STAR–2: A Soft Twisted-string-actuated Anthropomorphic Robotic Gripper: Design, Fabrication, and Preliminary Testing
                </a>
                <p>
                    <a style="color:#000;" href="https://www.linkedin.com/in/aaron-baker-310baa206/">Aaron Baker,</a>
                    <a style="color:#000;" href="">Claire Foy,</a>
                    <b>Steven Swanbeck</b>,
                    <a style="color:#000;" href="https://www.linkedin.com/in/revanthraghuram-konda-56bbb2158/">Revanth Konda,</a>
                    <a style="color:#000;" href="https://www.unr.edu/me/about/people/jun-zhang">Jun Zhang</a>
                    <br> IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM 2023) <br>
                    <b>Mar 2022 - Aug 2022</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://ieeexplore.ieee.org/abstract/document/10196149">Paper</a>
                    <li>2.0 version of previous soft gripper with increased range-of-motion and degrees-of-freedom.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/gripper_arm.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="https://github.com/steven-swanbeck/ROS_ur3ehand" class="research-proj-title">
                    Dexterous Soft Gripper Manipulator Integration for Human-Robot Interaction
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> Research Project <br>
                    <b>Feb 2022 - Aug 2022</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/steven-swanbeck/ROS_ur3ehand">Code</a>
                    <li>Enhanced version of anthropomorphic gripper with 5 additional degrees of freedom integrated with a UR3e manipulator.</li>
                    <li>ROS integration allowed for coordinated control of gripper and manipulator, enabling pick-and-place and human-robot interaction demonstrations.</li>
                </p>
            </div>

            <div>
                <a href="https://ieeexplore.ieee.org/abstract/document/9969176" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/gripper_tro.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="https://ieeexplore.ieee.org/abstract/document/9969176" class="research-proj-title">
                    Anthropomorphic Twisted String-Actuated Soft Robotic Gripper With Tendon-Based Stiffening
                </a>
                <p>
                    <a style="color:#000;" href="https://www.linkedin.com/in/revanthraghuram-konda-56bbb2158/">Revanth Konda*,</a>
                    <a style="color:#000;" href="https://www.linkedin.com/in/david-bombara-jr/">David Bombara*,</a>
                    <b>Steven Swanbeck*</b>,
                    <a style="color:#000;" href="https://www.unr.edu/me/about/people/jun-zhang">Jun Zhang</a>
                    <br> IEEE Transactions on Robotics (April 2023) <br>
                    <b>Sep 2021 - Feb 2022</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://ieeexplore.ieee.org/abstract/document/9969176">Paper</a> 
                    <li>Soft gripper capable of mimicking many of the grasping capabilities of the human hand, including achieving 31/33 grasps of the Feix GRASP Taxonomy and resisting a maximum force of 72N, over 13 times its own weight.</li>
                </p>
            </div>

            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <center>
                        <video width="320" height="200" playsinline autoplay loop muted>
                            <source src="assets/images/quadruped2.mp4" type="video/mp4">
                        </video>
                    </center>
                </a>
                <a href="" class="research-proj-title">
                    Sublunar Lava Tube Exploration Quadruped
                </a>
                <p>
                    <b>Steven Swanbeck</b>
                    <br> NASA University Student Design Challenge <br>
                    <b>May 2021 - Aug 2021</b>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/steven-swanbeck/SublunarExplorationQuadruped.git">Code</a>
                    <li>Project to design a robotic system capable of surveying lava tubes under the surface of the moon to assess their ability to sustain long-term human habitation.</li>
                    <li>Custom quarupedal system capable of teleoperated walking, self-stabilization, and LiDAR mapping of surroundings.</li>
                </p>
            </div>

            <div>
                <p>* indicates equal contribution</p>
            </div>

            <div style="clear: both;"></div>
        </div>
        <div class="divider"></div>
